{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0339322c",
   "metadata": {},
   "source": [
    "## PATH SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636aa887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST be first cell: set multiprocessing method for Windows\n",
    "import torch.multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "except RuntimeError:\n",
    "    pass  # Already set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d50292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: F:\\InfTech\\Prodotti\\Python\\GeoLocGit\\GeoLoc-CVCS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# FOR LOCAL USE THIS LINES\n",
    "current = Path.cwd()\n",
    "src_path = current / \"src\" if (current / \"src\").exists() else current.parent\n",
    "\n",
    "# FOR COLAB USE THIS LINE INSTEAD\n",
    "# BRANCH_NAME = \"main\"  # Change this to switch branches\n",
    "# !git clone -b {BRANCH_NAME} https://github.com/MatteoCamillo-code/GeoLoc-CVCS.git\n",
    "# !cd /content/GeoLoc-CVCS && git pull origin {BRANCH_NAME} && cd ..\n",
    "# src_path = Path(\"/content/GeoLoc-CVCS/src\").resolve()\n",
    "\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from utils.paths import find_project_root\n",
    "\n",
    "# Set working directory and sys.path properly\n",
    "project_root = find_project_root(src_path)\n",
    "data_dir = project_root / \"data\"\n",
    "history_dir = project_root / \"outputs\" / \"history\"\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "print(\"CWD:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f450636",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c976cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from configs.baseline_multi_head_ISN import TrainConfig\n",
    "\n",
    "cfg = TrainConfig()\n",
    "if cfg.backbone == \"resnet50\":\n",
    "    from torchvision.models import resnet50, ResNet50_Weights\n",
    "elif cfg.backbone == \"inceptionv4\":\n",
    "    import timm\n",
    "\n",
    "from utils.seed import seed_everything\n",
    "from utils.metrics import overall_val_acc_from_history\n",
    "from utils.io import save_json, delete_file\n",
    "from utils.paths import get_next_version, get_current_version\n",
    "from training.runner import fit\n",
    "from training.losses import CrossEntropyWithLabelSmoothing\n",
    "\n",
    "from src.utils.logging import get_logger\n",
    "from src.utils.paths import abs_path\n",
    "\n",
    "from models.multi_head_classifier import MultiHeadClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ad6ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed_everything(cfg.seed)\n",
    "\n",
    "device = cfg.device if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad64c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camil\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Path to dataset files: C:\\Users\\camil\\.cache\\kagglehub\\datasets\\josht000\\osv-mini-129k\\versions\\1/osv5m\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"josht000/osv-mini-129k\")\n",
    "path = path + \"/osv5m\"\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "image_root = path + \"/train_images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387b0940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Path to dataset files: C:\\Users\\camil\\.cache\\kagglehub\\datasets\\josht000\\osv-mini-129k\\versions\\1/osv5m\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"josht000/osv-mini-129k\")\n",
    "path = path + \"/osv5m\"\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "image_root = path + \"/train_images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f05014",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(cfg.scenes) == 1 and cfg.scenes[0] == \"total\":\n",
    "    train_val_path = data_dir / \"metadata/s2-geo-cells/train_val_split_geocells_total.csv\"\n",
    "else:\n",
    "    train_val_path = data_dir / \"metadata/s2-geo-cells/train_val_split_geocells.csv\"\n",
    "cell_centers_dfs = {}\n",
    "cells_hierarchy_dfs = {}\n",
    "\n",
    "for scene in cfg.scenes:\n",
    "    if cfg.same_partitions:\n",
    "        cell_centers_path = data_dir / f\"metadata/s2-geo-cells/cell_center_dataset_total.csv\"\n",
    "        cells_hierarchy_path = data_dir / f\"metadata/s2-geo-cells/cell_hierarchy_dataset_total.csv\"\n",
    "    else:\n",
    "        cell_centers_path = data_dir / f\"metadata/s2-geo-cells/cell_center_dataset_{scene}.csv\"\n",
    "        cells_hierarchy_path = data_dir / f\"metadata/s2-geo-cells/cell_hierarchy_dataset_{scene}.csv\"\n",
    "    \n",
    "    cell_centers_df = pd.read_csv(cell_centers_path)\n",
    "    cells_hierarchy_df = pd.read_csv(cells_hierarchy_path)\n",
    "    \n",
    "    # remove duplicates with same cell_id\n",
    "    cell_centers_df = cell_centers_df.drop_duplicates(subset='cell_id_token')\n",
    "\n",
    "    # Set S2 cell ID as index (assumes first column or 'cell_id' column)\n",
    "    if 'cell_id_token' in cell_centers_df.columns:\n",
    "        cell_centers_df = cell_centers_df.set_index('cell_id_token')\n",
    "    else:\n",
    "        # Set first column as index if it contains cell IDs\n",
    "        cell_centers_df = cell_centers_df.set_index(cell_centers_df.columns[0])\n",
    "        \n",
    "    cell_centers_dfs[scene] = cell_centers_df\n",
    "    cells_hierarchy_dfs[scene] = cells_hierarchy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662ad7d",
   "metadata": {},
   "source": [
    "## DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4888680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataloader_utils import create_dataloaders\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create all dataloaders with a single function call\n",
    "loader_dict = create_dataloaders(\n",
    "    image_root=image_root,\n",
    "    csv_path=train_val_path,\n",
    "    batch_size=cfg.batch_size,\n",
    "    num_workers=cfg.num_workers,\n",
    "    img_size=IMG_SIZE,\n",
    "    seed=cfg.seed,\n",
    "    train_subset_pct=cfg.train_size_pct,\n",
    "    val_subset_pct=cfg.val_size_pct,\n",
    "    scenes=cfg.scenes,\n",
    "    augment=True,\n",
    "    prefetch_factor=cfg.prefetch_factor,\n",
    "    persistent_workers=True if cfg.num_workers > 0 else False,\n",
    "    coarse_label_idx=cfg.coarse_label_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef1e57",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b36301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "def download_file_robust(url, filename):\n",
    "    \"\"\"Download a file if missing or corrupted.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        if \"pth.tar\" in filename and os.path.getsize(filename) < 90 * 1024 * 1024:\n",
    "            print(f\"File {filename} looks corrupt (too small). Re-downloading...\")\n",
    "            os.remove(filename)\n",
    "        else:\n",
    "            return\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    with urllib.request.urlopen(url) as response, open(filename, \"wb\") as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "\n",
    "def download_pretrained_on_places(model_name=\"resnet50\"):\n",
    "    \"\"\"Downloads a pre-trained Places365 model (ResNet50 or ResNet152).\"\"\"\n",
    "    url = f\"http://places2.csail.mit.edu/models_places365/{model_name}_places365.pth.tar\"\n",
    "    download_file_robust(url, f\"{model_name}_places365.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10203f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output classes urban: [1560, 884, 442]\n",
      "Output classes natural: [3253, 1761, 917]\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "for sc in cfg.scenes:\n",
    "    backbone = None\n",
    "    FEAT_DIM = 0\n",
    "    if cfg.backbone == \"inceptionv4\":\n",
    "        backbone = timm.create_model('inception_v4', pretrained=True)\n",
    "        FEAT_DIM = 1536  # inception_v4 feature dimension\n",
    "    elif cfg.backbone == \"resnet50\":\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "        backbone = resnet50(weights=weights)\n",
    "        FEAT_DIM = 2048  # resnet50 feature dimension  \n",
    "\n",
    "    # number of classes depends on partition\n",
    "    num_classes = list(map(\n",
    "        lambda idx: len(loader_dict[sc][\"label_maps\"][f\"label_config_{idx + 1}\"]),\n",
    "        cfg.coarse_label_idx\n",
    "    ))\n",
    "\n",
    "    backbone = nn.Sequential(\n",
    "        *list(backbone.children())[:-1],\n",
    "        nn.Flatten(1)\n",
    "    )\n",
    "\n",
    "\n",
    "    backbone = backbone.to(device)\n",
    "    # Optional: comment out if it causes issues on Windows/your PyTorch version\n",
    "    # model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "    model = MultiHeadClassifier(\n",
    "        backbone=backbone,\n",
    "        feat_dim=FEAT_DIM,\n",
    "        head_dims=num_classes,\n",
    "        dropout=cfg.dropout,\n",
    "        coarse_level_idx=cfg.coarse_label_idx,\n",
    "    ).to(device)\n",
    "    \n",
    "    models[sc] = model\n",
    "\n",
    "    print(f\"Output classes {sc}:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342d07d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized training components for 2 scenes: ['urban', 'natural']\n"
     ]
    }
   ],
   "source": [
    "criterion = CrossEntropyWithLabelSmoothing(ignore_index=-1, smoothing=cfg.label_smoothing)\n",
    "\n",
    "# Create optimizer, scheduler, and scaler for each model\n",
    "optimizers = {}\n",
    "schedulers = {}\n",
    "scalers = {}\n",
    "\n",
    "for scene, model in models.items():\n",
    "    optimizers[scene] = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    schedulers[scene] = StepLR(optimizers[scene], step_size=cfg.scheduler_step_size, gamma=cfg.scheduler_gamma)\n",
    "    scalers[scene] = torch.amp.GradScaler(device=cfg.device, enabled=cfg.amp)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"Initialized training components for {len(models)} scenes: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba589c1",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe789ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:41:46] INFO - Starting training MH_ISN_res_w for scene urban ...\n",
      "[14:42:41] INFO - Epoch 1/1 | train loss=6.5852 acc=1.52% | val loss=6.4510 acc=3.46% | geo acc={'acc@1km': '0.57%', 'acc@5km': '4.96%', 'acc@25km': '22.90%', 'acc@100km': '28.24%'} | time=55.4s\n",
      "[14:42:42] INFO - Training completed.\n",
      "[14:42:42] INFO - Starting training MH_ISN_res_w for scene natural ...\n",
      "[14:44:24] INFO - Epoch 1/1 | train loss=7.1181 acc=5.41% | val loss=6.7839 acc=11.62% | geo acc={'acc@1km': '1.96%', 'acc@5km': '8.84%', 'acc@25km': '18.71%', 'acc@100km': '30.60%'} | time=101.9s\n",
      "[14:44:25] INFO - Training completed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_json() got an unexpected keyword argument 'history_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     15\u001b[0m     history \u001b[38;5;241m=\u001b[39m fit(\n\u001b[0;32m     16\u001b[0m         cfg\u001b[38;5;241m=\u001b[39mcfg,\n\u001b[0;32m     17\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodels[scene],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m         version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     32\u001b[0m     histories[scene] \u001b[38;5;241m=\u001b[39m history\n\u001b[1;32m---> 34\u001b[0m \u001b[43msave_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scene \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mscenes:\n\u001b[0;32m     36\u001b[0m     base_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: save_json() got an unexpected keyword argument 'history_path'"
     ]
    }
   ],
   "source": [
    "# Train each scene's model separately\n",
    "histories = {}\n",
    "logger = get_logger(log_file=str(abs_path(cfg.output_dir, \"logs\", \"train.log\")))\n",
    "version = 0\n",
    "\n",
    "base_name = f\"{cfg.model_name}\"\n",
    "full_path = history_dir / f\"{base_name}_v{version}.json\"\n",
    "\n",
    "for scene in models.keys():\n",
    "    # Get versioned history path\n",
    "    base_name = f\"{cfg.model_name}_{scene}\"\n",
    "    version = get_next_version(history_dir, base_name)\n",
    "    history_path = history_dir / f\"{base_name}_v{version}.json\"\n",
    "    \n",
    "    history = fit(\n",
    "        cfg=cfg,\n",
    "        model=models[scene],\n",
    "        data_loader=loader_dict[scene],\n",
    "        cell_centers=cell_centers_dfs[scene],\n",
    "        cells_hierarchy=cells_hierarchy_dfs[scene],\n",
    "        optimizer=optimizers[scene],\n",
    "        criterion=criterion,\n",
    "        scaler=scalers[scene],\n",
    "        use_tqdm=cfg.use_tqdm,\n",
    "        scheduler=schedulers[scene],\n",
    "        logger=logger,\n",
    "        scene=scene,\n",
    "        history_path=history_path,\n",
    "        version=version,\n",
    "    )\n",
    "    \n",
    "    histories[scene] = history\n",
    "        \n",
    "save_json(obj=histories, path=full_path)\n",
    "for scene in cfg.scenes:\n",
    "    base_name = f\"{cfg.model_name}_{scene}\"\n",
    "    history_path = history_dir / f\"{base_name}_v{version}.json\"\n",
    "    delete_file(history_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe025d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\InfTech\\\\Prodotti\\\\Python\\\\GeoLocGit\\\\GeoLoc-CVCS\\\\outputs\\\\history\\\\baseline_multi_head_ISN_urban_v1_history.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# version = 3\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m accuracy_list \u001b[38;5;241m=\u001b[39m \u001b[43moverall_val_acc_from_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_list)\n",
      "File \u001b[1;32mf:\\InfTech\\Prodotti\\Python\\GeoLocGit\\GeoLoc-CVCS\\src\\utils\\metrics.py:34\u001b[0m, in \u001b[0;36moverall_val_acc_from_history\u001b[1;34m(cfg, project_root, version)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# collect\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scene \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mscenes:\n\u001b[1;32m---> 34\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscene\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_v\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mversion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_history.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     acc \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# last epoch\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     output[scene] \u001b[38;5;241m=\u001b[39m acc\n",
      "File \u001b[1;32mF:\\InfTech\\Prodotti\\Python\\GeoLocGit\\GeoLoc-CVCS\\src\\utils\\io.py:20\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_json\u001b[39m(path: Union[\u001b[38;5;28mstr\u001b[39m, Path]):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\tf\\lib\\pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1247\u001b[0m          errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;124;03m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;124;03m    the built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\tf\\lib\\pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\InfTech\\\\Prodotti\\\\Python\\\\GeoLocGit\\\\GeoLoc-CVCS\\\\outputs\\\\history\\\\baseline_multi_head_ISN_urban_v1_history.json'"
     ]
    }
   ],
   "source": [
    "# version = 3\n",
    "accuracy_list = overall_val_acc_from_history(cfg, project_root, version)\n",
    "print(accuracy_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
