{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0339322c",
      "metadata": {
        "id": "0339322c"
      },
      "source": [
        "## PATH SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "636aa887",
      "metadata": {
        "id": "636aa887"
      },
      "outputs": [],
      "source": [
        "# MUST be first cell: set multiprocessing method for Windows\n",
        "import torch.multiprocessing as mp\n",
        "try:\n",
        "    mp.set_start_method(\"spawn\", force=True)\n",
        "except RuntimeError:\n",
        "    pass  # Already set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6d50292",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6d50292",
        "outputId": "e7519ad1-0511-4206-9cc3-be1d55be9190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /content/GeoLoc-CVCS\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# FOR LOCAL USE THIS LINES\n",
        "# current = Path.cwd()\n",
        "# src_path = current / \"src\" if (current / \"src\").exists() else current.parent\n",
        "\n",
        "# FOR COLAB USE THIS LINE INSTEAD\n",
        "# BRANCH_NAME = \"weighted_loss\"  # Change this to switch branches\n",
        "# !git clone -b {BRANCH_NAME} https://github.com/MatteoCamillo-code/GeoLoc-CVCS.git\n",
        "# !cd /content/GeoLoc-CVCS && git pull origin {BRANCH_NAME} && cd ..\n",
        "src_path = Path(\"/content/GeoLoc-CVCS/src\").resolve()\n",
        "\n",
        "sys.path.insert(0, str(src_path))\n",
        "\n",
        "from utils.paths import find_project_root\n",
        "\n",
        "# Set working directory and sys.path properly\n",
        "project_root = find_project_root(src_path)\n",
        "data_dir = project_root / \"data\"\n",
        "history_dir = project_root / \"outputs\" / \"history\"\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "print(\"CWD:\", Path.cwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f450636",
      "metadata": {
        "id": "5f450636"
      },
      "source": [
        "## IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c976cd1f",
      "metadata": {
        "id": "c976cd1f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from configs.baseline_multi_head_ISN import TrainConfig\n",
        "\n",
        "cfg = TrainConfig()\n",
        "if cfg.backbone == \"resnet50\":\n",
        "    from torchvision.models import resnet50, ResNet50_Weights\n",
        "elif cfg.backbone == \"inceptionv4\":\n",
        "    import timm\n",
        "\n",
        "from utils.seed import seed_everything\n",
        "from utils.metrics import overall_val_acc_from_history\n",
        "from utils.io import save_json, delete_file\n",
        "from utils.paths import get_next_version, get_current_version\n",
        "from training.runner import fit\n",
        "from training.losses import CrossEntropyWithLabelSmoothing\n",
        "\n",
        "from src.utils.logging import get_logger\n",
        "from src.utils.paths import abs_path\n",
        "\n",
        "from models.multi_head_classifier import MultiHeadClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "06ad6ed4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ad6ed4",
        "outputId": "ca7ff804-ca4f-4c91-d68a-ff4b6d711404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "seed_everything(cfg.seed)\n",
        "\n",
        "device = cfg.device if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ad64c140",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad64c140",
        "outputId": "1535c1d7-e60f-4707-c8a3-7c6fe5595321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'osv-mini-129k' dataset.\n",
            "Path to dataset files: /kaggle/input/osv-mini-129k/osv5m\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"josht000/osv-mini-129k\")\n",
        "path = path + \"/osv5m\"\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "image_root = path + \"/train_images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f1f05014",
      "metadata": {
        "id": "f1f05014"
      },
      "outputs": [],
      "source": [
        "if len(cfg.scenes) == 1 and cfg.scenes[0] == \"total\":\n",
        "    train_val_path = data_dir / \"metadata/s2-geo-cells/train_val_split_geocells_total.csv\"\n",
        "else:\n",
        "    train_val_path = data_dir / \"metadata/s2-geo-cells/train_val_split_geocells.csv\"\n",
        "cell_centers_dfs = {}\n",
        "cells_hierarchy_dfs = {}\n",
        "\n",
        "for scene in cfg.scenes:\n",
        "    if cfg.same_partitions:\n",
        "        cell_centers_path = data_dir / f\"metadata/s2-geo-cells/cell_center_dataset_total.csv\"\n",
        "        cells_hierarchy_path = data_dir / f\"metadata/s2-geo-cells/cell_hierarchy_dataset_total.csv\"\n",
        "    else:\n",
        "        cell_centers_path = data_dir / f\"metadata/s2-geo-cells/cell_center_dataset_{scene}.csv\"\n",
        "        cells_hierarchy_path = data_dir / f\"metadata/s2-geo-cells/cell_hierarchy_dataset_{scene}.csv\"\n",
        "\n",
        "    cell_centers_df = pd.read_csv(cell_centers_path)\n",
        "    cells_hierarchy_df = pd.read_csv(cells_hierarchy_path)\n",
        "\n",
        "    # remove duplicates with same cell_id\n",
        "    cell_centers_df = cell_centers_df.drop_duplicates(subset='cell_id_token')\n",
        "\n",
        "    # Set S2 cell ID as index (assumes first column or 'cell_id' column)\n",
        "    if 'cell_id_token' in cell_centers_df.columns:\n",
        "        cell_centers_df = cell_centers_df.set_index('cell_id_token')\n",
        "    else:\n",
        "        # Set first column as index if it contains cell IDs\n",
        "        cell_centers_df = cell_centers_df.set_index(cell_centers_df.columns[0])\n",
        "\n",
        "    cell_centers_dfs[scene] = cell_centers_df\n",
        "    cells_hierarchy_dfs[scene] = cells_hierarchy_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9662ad7d",
      "metadata": {
        "id": "9662ad7d"
      },
      "source": [
        "## DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4888680e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4888680e",
        "outputId": "6b7de0ea-f687-4756-8b26-37bd49e0cd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from dataset.dataloader_utils import create_dataloaders\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create all dataloaders with a single function call\n",
        "loader_dict = create_dataloaders(\n",
        "    image_root=image_root,\n",
        "    csv_path=train_val_path,\n",
        "    batch_size=cfg.batch_size,\n",
        "    num_workers=cfg.num_workers,\n",
        "    img_size=IMG_SIZE,\n",
        "    seed=cfg.seed,\n",
        "    train_subset_pct=cfg.train_size_pct,\n",
        "    val_subset_pct=cfg.val_size_pct,\n",
        "    scenes=cfg.scenes,\n",
        "    augment=True,\n",
        "    prefetch_factor=cfg.prefetch_factor,\n",
        "    persistent_workers=True if cfg.num_workers > 0 else False,\n",
        "    coarse_label_idx=cfg.coarse_label_idx,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ef1e57",
      "metadata": {
        "id": "27ef1e57"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install s2sphere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN7Zal02Xlbt",
        "outputId": "7fefe6a0-d332-4583-a3ce-d218afa4613b"
      },
      "id": "aN7Zal02Xlbt",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: s2sphere in /usr/local/lib/python3.12/dist-packages (0.2.5)\n",
            "Requirement already satisfied: future>=0.15 in /usr/local/lib/python3.12/dist-packages (from s2sphere) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import s2sphere as s2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def compute_loss_weights(label_maps, coarse_label_idx):\n",
        "\n",
        "    weights = []\n",
        "\n",
        "    for idx in coarse_label_idx:\n",
        "        cell_list = label_maps[f\"label_config_{idx + 1}\"].to_list()\n",
        "\n",
        "        sum_of_areas = 0\n",
        "\n",
        "        for cell in cell_list:\n",
        "\n",
        "          cell_id = s2.CellId.from_token(cell)  # example\n",
        "          cell = s2.Cell(cell_id)\n",
        "\n",
        "          area_m2 = cell.exact_area()\n",
        "          area_km2 = area_m2 / 1e6\n",
        "\n",
        "          sum_of_areas += area_km2\n",
        "\n",
        "        weights.append(len(cell_list)/sum_of_areas)\n",
        "\n",
        "    return torch.tensor(np.array(weights)/sum(weights), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "wLTUuehyUYM2"
      },
      "id": "wLTUuehyUYM2",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c10203f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c10203f4",
        "outputId": "e45240a4-d318-4d90-d34f-fc9d2e21aa5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output classes urban: [1560, 884, 442]\n",
            "Output classes natural: [3253, 1761, 917]\n"
          ]
        }
      ],
      "source": [
        "models = {}\n",
        "loss_weights = {}\n",
        "\n",
        "for sc in cfg.scenes:\n",
        "    backbone = None\n",
        "    FEAT_DIM = 0\n",
        "    if cfg.backbone == \"inceptionv4\":\n",
        "        backbone = timm.create_model('inception_v4', pretrained=True)\n",
        "        FEAT_DIM = 1536  # inception_v4 feature dimension\n",
        "    elif cfg.backbone == \"resnet50\":\n",
        "        weights = ResNet50_Weights.IMAGENET1K_V2\n",
        "        backbone = resnet50(weights=weights)\n",
        "        FEAT_DIM = 2048  # resnet50 feature dimension\n",
        "\n",
        "    # number of classes depends on partition\n",
        "    num_classes = list(map(\n",
        "        lambda idx: len(loader_dict[sc][\"label_maps\"][f\"label_config_{idx + 1}\"]),\n",
        "        cfg.coarse_label_idx\n",
        "    ))\n",
        "\n",
        "    loss_weights[sc] = compute_loss_weights(loader_dict[sc][\"label_maps\"], cfg.coarse_label_idx)\n",
        "\n",
        "    backbone = nn.Sequential(\n",
        "        *list(backbone.children())[:-1],\n",
        "        nn.Flatten(1)\n",
        "    )\n",
        "\n",
        "\n",
        "    backbone = backbone.to(device)\n",
        "    # Optional: comment out if it causes issues on Windows/your PyTorch version\n",
        "    # model = torch.compile(model, backend=\"aot_eager\")\n",
        "\n",
        "    model = MultiHeadClassifier(\n",
        "        backbone=backbone,\n",
        "        feat_dim=FEAT_DIM,\n",
        "        head_dims=num_classes,\n",
        "        dropout=cfg.dropout,\n",
        "        coarse_level_idx=cfg.coarse_label_idx,\n",
        "    ).to(device)\n",
        "\n",
        "    models[sc] = model\n",
        "\n",
        "    print(f\"Output classes {sc}:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "342d07d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "342d07d4",
        "outputId": "ff6ee61b-2cbb-43c9-8c82-5b7938e77d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized training components for 2 scenes: ['urban', 'natural']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2063168552.py:11: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  scalers[scene] = torch.amp.GradScaler(device=cfg.device, enabled=cfg.amp)\n"
          ]
        }
      ],
      "source": [
        "criterion = CrossEntropyWithLabelSmoothing(ignore_index=-1, smoothing=cfg.label_smoothing)\n",
        "\n",
        "# Create optimizer, scheduler, and scaler for each model\n",
        "optimizers = {}\n",
        "schedulers = {}\n",
        "scalers = {}\n",
        "\n",
        "for scene, model in models.items():\n",
        "    optimizers[scene] = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    schedulers[scene] = StepLR(optimizers[scene], step_size=cfg.scheduler_step_size, gamma=cfg.scheduler_gamma)\n",
        "    scalers[scene] = torch.amp.GradScaler(device=cfg.device, enabled=cfg.amp)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(f\"Initialized training components for {len(models)} scenes: {list(models.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fba589c1",
      "metadata": {
        "id": "fba589c1"
      },
      "source": [
        "## TRAINING LOOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe789ac9",
      "metadata": {
        "id": "fe789ac9",
        "outputId": "9720353e-6bfc-49c2-c9a0-35126fd77376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[14:41:46] INFO - Starting training MH_ISN_res_w for scene urban ...\n",
            "[14:42:41] INFO - Epoch 1/1 | train loss=6.5852 acc=1.52% | val loss=6.4510 acc=3.46% | geo acc={'acc@1km': '0.57%', 'acc@5km': '4.96%', 'acc@25km': '22.90%', 'acc@100km': '28.24%'} | time=55.4s\n",
            "[14:42:42] INFO - Training completed.\n",
            "[14:42:42] INFO - Starting training MH_ISN_res_w for scene natural ...\n",
            "[14:44:24] INFO - Epoch 1/1 | train loss=7.1181 acc=5.41% | val loss=6.7839 acc=11.62% | geo acc={'acc@1km': '1.96%', 'acc@5km': '8.84%', 'acc@25km': '18.71%', 'acc@100km': '30.60%'} | time=101.9s\n",
            "[14:44:25] INFO - Training completed.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "save_json() got an unexpected keyword argument 'history_path'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     15\u001b[0m     history \u001b[38;5;241m=\u001b[39m fit(\n\u001b[0;32m     16\u001b[0m         cfg\u001b[38;5;241m=\u001b[39mcfg,\n\u001b[0;32m     17\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodels[scene],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m         version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     32\u001b[0m     histories[scene] \u001b[38;5;241m=\u001b[39m history\n\u001b[1;32m---> 34\u001b[0m \u001b[43msave_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scene \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mscenes:\n\u001b[0;32m     36\u001b[0m     base_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mTypeError\u001b[0m: save_json() got an unexpected keyword argument 'history_path'"
          ]
        }
      ],
      "source": [
        "# Train each scene's model separately\n",
        "histories = {}\n",
        "logger = get_logger(log_file=str(abs_path(cfg.output_dir, \"logs\", \"train.log\")))\n",
        "version = 0\n",
        "\n",
        "base_name = f\"{cfg.model_name}\"\n",
        "full_path = history_dir / f\"{base_name}_v{version}.json\"\n",
        "\n",
        "for scene in models.keys():\n",
        "    # Get versioned history path\n",
        "    base_name = f\"{cfg.model_name}_{scene}\"\n",
        "    version = get_next_version(history_dir, base_name)\n",
        "    history_path = history_dir / f\"{base_name}_v{version}.json\"\n",
        "\n",
        "    history = fit(\n",
        "        cfg=cfg,\n",
        "        model=models[scene],\n",
        "        data_loader=loader_dict[scene],\n",
        "        cell_centers=cell_centers_dfs[scene],\n",
        "        cells_hierarchy=cells_hierarchy_dfs[scene],\n",
        "        optimizer=optimizers[scene],\n",
        "        criterion=criterion,\n",
        "        loss_weights=loss_weights[scene],\n",
        "        scaler=scalers[scene],\n",
        "        use_tqdm=cfg.use_tqdm,\n",
        "        scheduler=schedulers[scene],\n",
        "        logger=logger,\n",
        "        scene=scene,\n",
        "        history_path=history_path,\n",
        "        version=version,\n",
        "    )\n",
        "\n",
        "    histories[scene] = history\n",
        "\n",
        "save_json(obj=histories, path=full_path)\n",
        "for scene in cfg.scenes:\n",
        "    base_name = f\"{cfg.model_name}_{scene}\"\n",
        "    history_path = history_dir / f\"{base_name}_v{version}.json\"\n",
        "    delete_file(history_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81fe025d",
      "metadata": {
        "id": "81fe025d",
        "outputId": "10d0c0be-ba46-4466-b525-34ac011c6898"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'F:\\\\InfTech\\\\Prodotti\\\\Python\\\\GeoLocGit\\\\GeoLoc-CVCS\\\\outputs\\\\history\\\\baseline_multi_head_ISN_urban_v1_history.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# version = 3\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m accuracy_list \u001b[38;5;241m=\u001b[39m \u001b[43moverall_val_acc_from_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_list)\n",
            "File \u001b[1;32mf:\\InfTech\\Prodotti\\Python\\GeoLocGit\\GeoLoc-CVCS\\src\\utils\\metrics.py:34\u001b[0m, in \u001b[0;36moverall_val_acc_from_history\u001b[1;34m(cfg, project_root, version)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# collect\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scene \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mscenes:\n\u001b[1;32m---> 34\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscene\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_v\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mversion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_history.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     acc \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# last epoch\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     output[scene] \u001b[38;5;241m=\u001b[39m acc\n",
            "File \u001b[1;32mF:\\InfTech\\Prodotti\\Python\\GeoLocGit\\GeoLoc-CVCS\\src\\utils\\io.py:20\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_json\u001b[39m(path: Union[\u001b[38;5;28mstr\u001b[39m, Path]):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
            "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\tf\\lib\\pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1247\u001b[0m          errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;124;03m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;124;03m    the built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\tf\\lib\\pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\InfTech\\\\Prodotti\\\\Python\\\\GeoLocGit\\\\GeoLoc-CVCS\\\\outputs\\\\history\\\\baseline_multi_head_ISN_urban_v1_history.json'"
          ]
        }
      ],
      "source": [
        "# version = 3\n",
        "accuracy_list = overall_val_acc_from_history(cfg, project_root, version)\n",
        "print(accuracy_list)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}